!_TAG_EXTRA_DESCRIPTION	anonymous	/Include tags for non-named objects like lambda/
!_TAG_EXTRA_DESCRIPTION	fileScope	/Include tags of file scope/
!_TAG_EXTRA_DESCRIPTION	pseudo	/Include pseudo tags/
!_TAG_EXTRA_DESCRIPTION	subparser	/Include tags generated by subparsers/
!_TAG_FIELD_DESCRIPTION	epoch	/the last modified time of the input file (only for F\/file kind tag)/
!_TAG_FIELD_DESCRIPTION	file	/File-restricted scoping/
!_TAG_FIELD_DESCRIPTION	input	/input file/
!_TAG_FIELD_DESCRIPTION	name	/tag name/
!_TAG_FIELD_DESCRIPTION	pattern	/pattern/
!_TAG_FIELD_DESCRIPTION	typeref	/Type and name of a variable or typedef/
!_TAG_FIELD_DESCRIPTION!Python	nameref	/the original name for the tag/
!_TAG_FILE_FORMAT	2	/extended format; --format=1 will not append ;" to lines/
!_TAG_FILE_SORTED	1	/0=unsorted, 1=sorted, 2=foldcase/
!_TAG_KIND_DESCRIPTION!Markdown	S,subsection	/level 2 sections/
!_TAG_KIND_DESCRIPTION!Markdown	T,l4subsection	/level 4 sections/
!_TAG_KIND_DESCRIPTION!Markdown	c,chapter	/chapters/
!_TAG_KIND_DESCRIPTION!Markdown	h,hashtag	/hashtags/
!_TAG_KIND_DESCRIPTION!Markdown	n,footnote	/footnotes/
!_TAG_KIND_DESCRIPTION!Markdown	s,section	/sections/
!_TAG_KIND_DESCRIPTION!Markdown	t,subsubsection	/level 3 sections/
!_TAG_KIND_DESCRIPTION!Markdown	u,l5subsection	/level 5 sections/
!_TAG_KIND_DESCRIPTION!Python	I,namespace	/name referring a module defined in other file/
!_TAG_KIND_DESCRIPTION!Python	Y,unknown	/name referring a class\/variable\/function\/module defined in other module/
!_TAG_KIND_DESCRIPTION!Python	c,class	/classes/
!_TAG_KIND_DESCRIPTION!Python	f,function	/functions/
!_TAG_KIND_DESCRIPTION!Python	i,module	/modules/
!_TAG_KIND_DESCRIPTION!Python	m,member	/class members/
!_TAG_KIND_DESCRIPTION!Python	v,variable	/variables/
!_TAG_OUTPUT_EXCMD	mixed	/number, pattern, mixed, or combineV2/
!_TAG_OUTPUT_FILESEP	slash	/slash or backslash/
!_TAG_OUTPUT_MODE	u-ctags	/u-ctags or e-ctags/
!_TAG_OUTPUT_VERSION	0.0	/current.age/
!_TAG_PARSER_VERSION!Markdown	1.1	/current.age/
!_TAG_PARSER_VERSION!Python	0.0	/current.age/
!_TAG_PATTERN_LENGTH_LIMIT	96	/0 for no limit/
!_TAG_PROC_CWD	/home/odinhg/Python/nrk_former_dql/	//
!_TAG_PROGRAM_AUTHOR	Universal Ctags Team	//
!_TAG_PROGRAM_NAME	Universal Ctags	/Derived from Exuberant Ctags/
!_TAG_PROGRAM_URL	https://ctags.io/	/official site/
!_TAG_PROGRAM_VERSION	6.1.0	/v6.1.0/
!_TAG_ROLE_DESCRIPTION!Python!module	imported	/imported modules/
!_TAG_ROLE_DESCRIPTION!Python!module	indirectlyImported	/module imported in alternative name/
!_TAG_ROLE_DESCRIPTION!Python!module	namespace	/namespace from where classes\/variables\/functions are imported/
!_TAG_ROLE_DESCRIPTION!Python!unknown	imported	/imported from the other module/
!_TAG_ROLE_DESCRIPTION!Python!unknown	indirectlyImported	/classes\/variables\/functions\/modules imported in alternative name/
BATCH_SIZE	dqn.py	/^BATCH_SIZE = 128#64 $/;"	v
BG_COLOR	environment.py	/^BG_COLOR = Color("#000000")$/;"	v
Board	environment.py	/^class Board:$/;"	c
COLORS	environment.py	/^COLORS = {$/;"	v
CURSOR_COLOR	environment.py	/^CURSOR_COLOR = Color("#ff0000")$/;"	v
EMPTY	environment.py	/^EMPTY = " "$/;"	v
EPS_DECAY	dqn.py	/^EPS_DECAY = 10000 $/;"	v
EPS_END	dqn.py	/^EPS_END = 0.05$/;"	v
EPS_START	dqn.py	/^EPS_START = 0.999$/;"	v
GAMMA	dqn.py	/^GAMMA = 0.99$/;"	v
LR	dqn.py	/^LR = 1e-5$/;"	v
Model	model.py	/^class Model(nn.Module):$/;"	c
Q_values	dqn.py	/^        Q_values = main_network(state_batch).gather(1, action_batch)$/;"	v
ReplayMemory	replay_memory.py	/^class ReplayMemory(object):$/;"	c
SYMBOLS	environment.py	/^SYMBOLS = ["B", "G", "P", "O"]$/;"	v
SYMBOL_TO_INDEX	environment.py	/^SYMBOL_TO_INDEX = {symbol: i for i, symbol in enumerate([EMPTY] + SYMBOLS)}$/;"	v
Solving NRK's "Former" Puzzle Game Using Deep Q-Learning	README.md	/^# Solving NRK's "Former" Puzzle Game Using Deep Q-Learning$/;"	c
TAU	dqn.py	/^TAU = 0.001$/;"	v
Transition	replay_memory.py	/^Transition = namedtuple("Transition", ("state", "action", "reward", "next_state", "is_terminal")/;"	v
__init__	environment.py	/^    def __init__(self, width: int | None = None, height: int | None = None, filename: str | None/;"	m	class:Board
__init__	model.py	/^    def __init__(self, width, height):$/;"	m	class:Model
__init__	replay_memory.py	/^    def __init__(self, capacity):$/;"	m	class:ReplayMemory
__len__	replay_memory.py	/^    def __len__(self):$/;"	m	class:ReplayMemory
action	dqn.py	/^    action = select_action(state, epsilon, main_network)$/;"	v
action_batch	dqn.py	/^        action_batch = torch.tensor(batch.action).unsqueeze(1)$/;"	v
batch	dqn.py	/^        batch = Transition(*zip(*transitions))$/;"	v
board	dqn.py	/^board = Board(width, height)$/;"	v
board	environment.py	/^    board = Board(3, 4)$/;"	v
click	environment.py	/^    def click(self, index):$/;"	m	class:Board
coords_to_index	environment.py	/^    def coords_to_index(self, x, y):$/;"	m	class:Board
count_non_empty_blocks	environment.py	/^def count_non_empty_blocks(board):$/;"	f
destroy_blocks	environment.py	/^def destroy_blocks(board, x, y):$/;"	f
episode_reward	dqn.py	/^        episode_reward = 0$/;"	v
episode_reward	dqn.py	/^episode_reward = 0$/;"	v
episode_rewards	dqn.py	/^episode_rewards = []$/;"	v
epsilon	dqn.py	/^        epsilon = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * n_episodes \/ EPS_DECAY)$/;"	v
epsilon	dqn.py	/^epsilon = EPS_START$/;"	v
forward	model.py	/^    def forward(self, x):$/;"	m	class:Model
generate_random_board	environment.py	/^def generate_random_board(width, height):$/;"	f
get_encoded_board	environment.py	/^    def get_encoded_board(self):$/;"	m	class:Board
height	dqn.py	/^width, height = 3, 4#7, 9$/;"	v
index_to_coords	environment.py	/^    def index_to_coords(self, index):$/;"	m	class:Board
init_weights	model.py	/^    def init_weights(self):$/;"	m	class:Model
is_game_over	environment.py	/^    def is_game_over(self):$/;"	m	class:Board
is_not_empty	environment.py	/^    def is_not_empty(self, x, y):$/;"	m	class:Board
is_terminal	dqn.py	/^    is_terminal = board.is_game_over()$/;"	v
is_terminal_batch	dqn.py	/^        is_terminal_batch = torch.tensor(batch.is_terminal).int().unsqueeze(1)$/;"	v
is_within_bounds	environment.py	/^def is_within_bounds(board, x, y):$/;"	f
loss	dqn.py	/^        loss = torch.nn.functional.smooth_l1_loss(Q_values, target_Q_values)$/;"	v
losses	dqn.py	/^            losses = []$/;"	v
losses	dqn.py	/^losses = []$/;"	v
main_network	dqn.py	/^main_network = Model(width, height)$/;"	v
main_network_state_dict	dqn.py	/^        main_network_state_dict = main_network.state_dict()$/;"	v
mean_episode_reward	dqn.py	/^            mean_episode_reward = sum(episode_rewards) \/ len(episode_rewards)$/;"	v
mean_loss	dqn.py	/^            mean_loss = sum(losses) \/ len(losses)$/;"	v
mean_moves_used	dqn.py	/^            mean_moves_used = sum(moves_used) \/ len(moves_used)$/;"	v
memory	dqn.py	/^memory = ReplayMemory(5000)$/;"	v
move_blocks_down	environment.py	/^def move_blocks_down(board):$/;"	f
moves_used	dqn.py	/^            moves_used = []$/;"	v
moves_used	dqn.py	/^moves_used = []$/;"	v
n_actions	dqn.py	/^n_actions = width * height$/;"	v
n_episodes	dqn.py	/^n_episodes = 0$/;"	v
next_state	dqn.py	/^    next_state = torch.tensor(board.get_encoded_board()).float().unsqueeze(0)$/;"	v
next_state_batch	dqn.py	/^        next_state_batch = torch.cat(batch.next_state)$/;"	v
nn	model.py	/^import torch.nn as nn$/;"	I	nameref:module:torch.nn
number_of_blocks_removed	environment.py	/^def number_of_blocks_removed(board1, board2): $/;"	f
optimizer	dqn.py	/^optimizer = torch.optim.Adam(main_network.parameters(), lr=LR)$/;"	v
push	replay_memory.py	/^    def push(self, *args):$/;"	m	class:ReplayMemory
read_board_from_file	environment.py	/^def read_board_from_file(filename):$/;"	f
render	environment.py	/^    def render(self, block_size=50):$/;"	m	class:Board
reset	environment.py	/^    def reset(self):$/;"	m	class:Board
reward	dqn.py	/^    reward = reward_function(state, action, next_state, is_terminal)$/;"	v
reward_batch	dqn.py	/^        reward_batch = torch.tensor(batch.reward).unsqueeze(1)$/;"	v
reward_function	dqn.py	/^def reward_function(state, action, next_state, is_terminal):$/;"	f
sample	replay_memory.py	/^    def sample(self, batch_size):$/;"	m	class:ReplayMemory
select_action	dqn.py	/^def select_action(state, epsilon, main_network):$/;"	f
state	dqn.py	/^    state = torch.tensor(board.get_encoded_board()).float().unsqueeze(0)$/;"	v
state_batch	dqn.py	/^        state_batch = torch.cat(batch.state)$/;"	v
target_Q_values	dqn.py	/^        target_Q_values = reward_batch + GAMMA * target_network(next_state_batch).max(dim=1, kee/;"	v
target_network	dqn.py	/^target_network = Model(width, height)$/;"	v
target_state_dict	dqn.py	/^        target_state_dict = target_network.state_dict()$/;"	v
transitions	dqn.py	/^        transitions = memory.sample(BATCH_SIZE)$/;"	v
width	dqn.py	/^width, height = 3, 4#7, 9$/;"	v
